{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.llms.databricks import Databricks\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "from pinecone import Pinecone\n",
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API keys setup\n",
    "DATABRICKS_TOKEN = os.environ.get('DATABRICKS_TOKEN')\n",
    "COHERE_API_KEY = os.environ.get('COHERE_API_KEY')\n",
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
    "\n",
    "# Setup Databricks model for LLM\n",
    "Settings.llm = Databricks(\n",
    "    model=\"databricks-meta-llama-3-1-70b-instruct\",\n",
    "    api_key=DATABRICKS_TOKEN,\n",
    "    api_base=\"https://adb-7215147325717155.15.azuredatabricks.net/serving-endpoints\",\n",
    ")\n",
    "\n",
    "# Setup embedding model\n",
    "Settings.embed_model = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    ")\n",
    "\n",
    "# Setup Pinecone vector store\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "pinecone_index = pc.Index(\"sidindex\")\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index, text_key=\"text\")\n",
    "\n",
    "# Setup Cohere reranker\n",
    "cohere_rerank = CohereRerank(api_key=COHERE_API_KEY, top_n=4)\n",
    "\n",
    "# Initialize the query engine with the vector store and reranker\n",
    "query_engine = VectorStoreIndex.from_vector_store(vector_store).as_query_engine(\n",
    "    similarity_top_k=10, node_postprocessors=[cohere_rerank]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.agent_toolkits import create_sql_agent\n",
    "\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///demo.db\")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "sql_agent = create_sql_agent(\n",
    "    llm, db = db, agent_type = \"openai-tools\", verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "choices = [\n",
    "    \"Utile pour répondre aux questions sur les politiques de soutien, l'innovation et les scénarios prospectifs.\",\n",
    "    \"Utile pour répondre aux questions sur des données precises.\"\n",
    "]\n",
    "\n",
    "choices_str = \"\\n\\n\".join([f\"{idx+1}. {choice}\" for idx, choice in enumerate(choices)])\n",
    "\n",
    "router_prompt_template = PromptTemplate(\n",
    "    \"Some choices are given below. It is provided in a numbered list (1 to\"\n",
    "    \" {num_choices}), where each item in the list corresponds to a summary.\\n\"\n",
    "    \"---------------------\\n{context_list}\\n---------------------\\nUsing only the choices\"\n",
    "    \" above and not prior knowledge, return the top choice that is most relevant\"\n",
    "    \" to the question: '{query_str}'\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import json \n",
    "\n",
    "class Answer(BaseModel):\n",
    "    choice: int\n",
    "    reason: str\n",
    "\n",
    "class RouterOutputParser:\n",
    "    def parse(self, output: str) -> Answer:\n",
    "        output = output.strip()\n",
    "        json_output = output[output.find(\"[\"):output.find(\"]\")+1]\n",
    "        json_dict = json.loads(json_output)[0]\n",
    "        return Answer(choice=json_dict[\"choice\"], reason=json_dict[\"reason\"])\n",
    "\n",
    "    def format(self, prompt_template: str) -> str:\n",
    "        format_str = (\n",
    "            \"\"\"The output should be formatted as a JSON instance that conforms to the JSON schema below. \n",
    "            {\n",
    "              \"type\": \"array\",\n",
    "              \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                  \"choice\": {\n",
    "                    \"type\": \"integer\"\n",
    "                  },\n",
    "                  \"reason\": {\n",
    "                    \"type\": \"string\"\n",
    "                  }\n",
    "                },\n",
    "                \"required\": [\n",
    "                  \"choice\",\n",
    "                  \"reason\"\n",
    "                ],\n",
    "                \"additionalProperties\": false\n",
    "              }\n",
    "            }\n",
    "            \"\"\"\n",
    "        )\n",
    "        return prompt_template + \"\\n\\n\" + format_str.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "output_parser = RouterOutputParser()\n",
    "\n",
    "def route_query(query_str: str):\n",
    "    fmt_prompt = router_prompt_template.format(\n",
    "        num_choices=len(choices),\n",
    "        context_list=choices_str,\n",
    "        query_str=query_str,\n",
    "        max_outputs=1\n",
    "    )\n",
    "    fmt_json_prompt = output_parser.format(fmt_prompt)\n",
    "    raw_output = llm.complete(fmt_json_prompt)\n",
    "    parsed = output_parser.parse(str(raw_output))\n",
    "\n",
    "    if parsed.choice == 1:\n",
    "        return query_engine.query(query_str)\n",
    "    elif parsed.choice == 2:\n",
    "        return sql_agent.invoke(query_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = route_query(\"Quelles sont les 4 scenarios prospectifs stratégiques étudiés pour l'Algérie en 2035?\")\n",
    "print(response)\n",
    "\n",
    "response = route_query(\"De combien le pourcentage de terres arables aménagées pour l’irrigation a évolué en 2019?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
