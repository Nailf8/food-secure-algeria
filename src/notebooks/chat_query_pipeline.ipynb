{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NailFerroukhi\\miniconda3\\envs\\sid\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\NailFerroukhi\\miniconda3\\envs\\sid\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.llms.databricks import Databricks\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "from pinecone import Pinecone\n",
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API keys setup\n",
    "DATABRICKS_TOKEN = os.environ.get('DATABRICKS_TOKEN')\n",
    "COHERE_API_KEY = os.environ.get('COHERE_API_KEY')\n",
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
    "\n",
    "# Setup embedding model\n",
    "Settings.embed_model = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    ")\n",
    "\n",
    "# Setup Pinecone vector store\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "pinecone_index = pc.Index(\"sidindex\")\n",
    "index = PineconeVectorStore(pinecone_index=pinecone_index, text_key=\"text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import InputComponent, QueryPipeline, ArgPackComponent\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from scripts.pipeline import ChatPipeline, ResponseWithChatHistory\n",
    "\n",
    "\n",
    "\n",
    "# First, we create an input component to capture the user query\n",
    "input_component = InputComponent()\n",
    "\n",
    "# Next, we use the LLM to rewrite a user query\n",
    "rewrite = (\n",
    "    \"Please write a query to a semantic search engine using the current conversation.\\n\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    \"{chat_history_str}\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    \"Latest message: {query_str}\\n\"\n",
    "    'Query:\"\"\"\\n'\n",
    ")\n",
    "rewrite_template = PromptTemplate(rewrite)\n",
    "\n",
    "llm = Databricks(\n",
    "    model=\"databricks-meta-llama-3-1-70b-instruct\",\n",
    "    api_key=DATABRICKS_TOKEN,\n",
    "    api_base=\"https://adb-7215147325717155.15.azuredatabricks.net/serving-endpoints\",\n",
    ")\n",
    "\n",
    "# using that, we will retrieve...\n",
    "retriever = VectorStoreIndex.from_vector_store(index).as_retriever(similarity_top_k=15)\n",
    "\n",
    "# we will retrieve two times, so we need to pack the retrieved nodes into a single list\n",
    "argpack_component = ArgPackComponent()\n",
    "\n",
    "# then postprocess/rerank with Cohere reranker\n",
    "reranker = CohereRerank(api_key=COHERE_API_KEY, top_n=10)\n",
    "\n",
    "\n",
    "response_component = ResponseWithChatHistory(\n",
    "    llm=llm,\n",
    "    system_prompt=(\n",
    "        \"You are a Q&A system. You will be provided with the previous chat history, \"\n",
    "        \"as well as possibly relevant context, to assist in answering a user message.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "p = QueryPipeline(verbose=False)\n",
    "p.add_modules(\n",
    "            {\n",
    "                \"input\": input_component,\n",
    "                \"rewrite_template\": rewrite_template,\n",
    "                \"llm\": llm,\n",
    "                \"retriever\": retriever,\n",
    "                \"reranker\": reranker,\n",
    "                \"response_component\": response_component,\n",
    "            }\n",
    "        )\n",
    "p.add_link(\"input\", \"rewrite_template\", src_key=\"query_str\", dest_key=\"query_str\")\n",
    "p.add_link(\"input\", \"rewrite_template\", src_key=\"chat_history_str\", dest_key=\"chat_history_str\")\n",
    "        \n",
    "# Linking rewrite_template to LLM, and LLM to retriever\n",
    "p.add_link(\"rewrite_template\", \"llm\")\n",
    "p.add_link(\"llm\", \"retriever\")\n",
    "\n",
    "# Linking retriever to reranker, and LLM to reranker\n",
    "p.add_link(\"retriever\", \"reranker\", dest_key=\"nodes\")\n",
    "p.add_link(\"llm\", \"reranker\", dest_key=\"query_str\")\n",
    "\n",
    "# Linking reranker to response_component\n",
    "p.add_link(\"reranker\", \"response_component\", dest_key=\"nodes\")\n",
    "\n",
    "# Linking input to response_component\n",
    "p.add_link(\"input\", \"response_component\", src_key=\"query_str\", dest_key=\"query_str\")\n",
    "p.add_link(\"input\", \"response_component\", src_key=\"chat_history\", dest_key=\"chat_history\")\n",
    "\n",
    "pipeline_memory = ChatMemoryBuffer.from_defaults(token_limit=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = QueryPipeline(\n",
    "    modules={\n",
    "        \"input\": input_component,\n",
    "        \"rewrite_template\": rewrite_template,\n",
    "        \"llm\": llm,\n",
    "        \"rewrite_retriever\": retriever,\n",
    "        \"query_retriever\": retriever,\n",
    "        \"join\": argpack_component,\n",
    "        \"reranker\": reranker,\n",
    "        \"response_component\": response_component,\n",
    "    },\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# run both retrievers -- once with the hallucinated query, once with the real query\n",
    "pipeline.add_link(\n",
    "    \"input\", \"rewrite_template\", src_key=\"query_str\", dest_key=\"query_str\"\n",
    ")\n",
    "pipeline.add_link(\n",
    "    \"input\",\n",
    "    \"rewrite_template\",\n",
    "    src_key=\"chat_history_str\",\n",
    "    dest_key=\"chat_history_str\",\n",
    ")\n",
    "pipeline.add_link(\"rewrite_template\", \"llm\")\n",
    "pipeline.add_link(\"llm\", \"rewrite_retriever\")\n",
    "pipeline.add_link(\"input\", \"query_retriever\", src_key=\"query_str\")\n",
    "\n",
    "# each input to the argpack component needs a dest key -- it can be anything\n",
    "# then, the argpack component will pack all the inputs into a single list\n",
    "pipeline.add_link(\"rewrite_retriever\", \"join\", dest_key=\"rewrite_nodes\")\n",
    "pipeline.add_link(\"query_retriever\", \"join\", dest_key=\"query_nodes\")\n",
    "\n",
    "# reranker needs the packed nodes and the query string\n",
    "pipeline.add_link(\"join\", \"reranker\", dest_key=\"nodes\")\n",
    "pipeline.add_link(\n",
    "    \"input\", \"reranker\", src_key=\"query_str\", dest_key=\"query_str\"\n",
    ")\n",
    "\n",
    "# synthesizer needs the reranked nodes and query str\n",
    "pipeline.add_link(\"reranker\", \"response_component\", dest_key=\"nodes\")\n",
    "pipeline.add_link(\n",
    "    \"input\", \"response_component\", src_key=\"query_str\", dest_key=\"query_str\"\n",
    ")\n",
    "pipeline.add_link(\n",
    "    \"input\",\n",
    "    \"response_component\",\n",
    "    src_key=\"chat_history\",\n",
    "    dest_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selon le Context Chunk 4, les politiques de soutien √† la p√™che en Alg√©rie ont inclus :\n",
      "\n",
      "* Le Plan National du D√©veloppement de la P√™che et de l'Aquaculture (PNDPA) 2003-2007\n",
      "* Le Sch√©ma Directeur de D√©veloppement des Activit√©s de la P√™che et de l'Aquaculture (SDDAPA) 2025\n",
      "* La feuille de route Strat-Sa√Ød (2012-2014)\n",
      "* Le plan Aqua-P√™che 2020\n",
      "\n",
      "Ces politiques ont vis√© √† promouvoir le d√©veloppement de la p√™che et de l'aquaculture en Alg√©rie, en mettant en place des m√©canismes de soutien pour les p√™cheurs et les entreprises du secteur.\n"
     ]
    }
   ],
   "source": [
    "response = pipeline._execute(\"Quelles ont √©t√© les politiques de soutien √† la p√™che en Algerie?\", pipeline_memory)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pipeline._execute(\"hello\", pipeline_memory=pipeline_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Bonjour! Comment puis-je vous aider aujourd'hui?\n",
      "assistant: La s√©curit√© alimentaire en Alg√©rie est un sujet complexe et multiforme. Selon les contextes fournis, il semble que la s√©curit√© alimentaire en Alg√©rie est confront√©e √† plusieurs d√©fis. \n",
      "\n",
      "D'une part, les politiques agricoles et de soutien √† l'agriculture ont √©t√© mises en place pour am√©liorer la s√©curit√© alimentaire, mais leur efficacit√© et leur impact sont discut√©s. Le Plan National de D√©veloppement Agricole (PNDA) de 2000 visait √† am√©liorer la s√©curit√© alimentaire, mais son √©valuation est difficile en raison du manque de donn√©es et de documents formels.\n",
      "\n",
      "D'autre part, les √©tudes et les recherches sur la s√©curit√© alimentaire en Alg√©rie sont nombreuses, mais elles montrent que le sujet est encore peu explor√©, notamment en ce qui concerne les effets des politiques de soutien √† l'agriculture et √† la p√™che sur la s√©curit√© alimentaire.\n",
      "\n",
      "En outre, les d√©fis li√©s √† la s√©curit√© alimentaire en Alg√©rie sont nombreux, tels que la p√©nurie de main-d'≈ìuvre agricole, l'urbanisation anarchique, la consommation des terres agricoles les plus fertiles, les difficult√©s d'approvisionnement en intrants, etc.\n",
      "\n",
      "En r√©sum√©, la s√©curit√© alimentaire en Alg√©rie est un sujet complexe qui n√©cessite une approche globale et une meilleure compr√©hension des d√©fis et des opportunit√©s. Les politiques et les strat√©gies mises en place doivent √™tre √©valu√©es et ajust√©es pour r√©pondre aux besoins de la population et assurer une s√©curit√© alimentaire durable.\n",
      "assistant: Your previous question was: How is food security in Algeria?\n"
     ]
    }
   ],
   "source": [
    "msg = [\"hello\", \"how is food security in Algeria?\", \"What was my previous question?\"]\n",
    "\n",
    "for msg in msg:\n",
    "    response = str(pipeline._run(msg, pipeline_memory=pipeline_memory))\n",
    "    response.replace(\"assistant:\", \"\").strip()\n",
    "    print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(pipeline.dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the network\n",
    "net.write_html(\"../data/html/2_qpchat_dag.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [phoenix.session.session] Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "# setup Arize Phoenix for logging/observability\n",
    "import phoenix as px\n",
    "import llama_index.core\n",
    "\n",
    "px.launch_app()\n",
    "llama_index.core.set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
