{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NailFerroukhi\\miniconda3\\envs\\sid\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\NailFerroukhi\\miniconda3\\envs\\sid\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.llms.databricks import Databricks\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "from pinecone import Pinecone\n",
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API keys setup\n",
    "DATABRICKS_TOKEN = os.environ.get('DATABRICKS_TOKEN')\n",
    "COHERE_API_KEY = os.environ.get('COHERE_API_KEY')\n",
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
    "\n",
    "# Setup embedding model\n",
    "Settings.embed_model = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    ")\n",
    "\n",
    "# Setup Pinecone vector store\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "pinecone_index = pc.Index(\"sidindex\")\n",
    "index = PineconeVectorStore(pinecone_index=pinecone_index, text_key=\"text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import InputComponent, QueryPipeline, ArgPackComponent\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from scripts.pipeline import ChatPipeline, ResponseWithChatHistory\n",
    "\n",
    "\n",
    "\n",
    "# First, we create an input component to capture the user query\n",
    "input_component = InputComponent()\n",
    "\n",
    "# Next, we use the LLM to rewrite a user query\n",
    "rewrite = (\n",
    "    \"Please write a query to a semantic search engine using the current conversation.\\n\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    \"{chat_history_str}\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    \"Latest message: {query_str}\\n\"\n",
    "    'Query:\"\"\"\\n'\n",
    ")\n",
    "rewrite_template = PromptTemplate(rewrite)\n",
    "\n",
    "llm = Databricks(\n",
    "    model=\"databricks-meta-llama-3-1-70b-instruct\",\n",
    "    api_key=DATABRICKS_TOKEN,\n",
    "    api_base=\"https://adb-7215147325717155.15.azuredatabricks.net/serving-endpoints\",\n",
    ")\n",
    "\n",
    "# using that, we will retrieve...\n",
    "retriever = VectorStoreIndex.from_vector_store(index).as_retriever(similarity_top_k=15)\n",
    "\n",
    "# we will retrieve two times, so we need to pack the retrieved nodes into a single list\n",
    "argpack_component = ArgPackComponent()\n",
    "\n",
    "# then postprocess/rerank with Cohere reranker\n",
    "reranker = CohereRerank(api_key=COHERE_API_KEY, top_n=10)\n",
    "\n",
    "\n",
    "response_component = ResponseWithChatHistory(\n",
    "    llm=llm,\n",
    "    system_prompt=(\n",
    "        \"You are a Q&A system. You will be provided with the previous chat history, \"\n",
    "        \"as well as possibly relevant context, to assist in answering a user message.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "p = QueryPipeline(verbose=False)\n",
    "p.add_modules(\n",
    "            {\n",
    "                \"input\": input_component,\n",
    "                \"rewrite_template\": rewrite_template,\n",
    "                \"llm\": llm,\n",
    "                \"retriever\": retriever,\n",
    "                \"reranker\": reranker,\n",
    "                \"response_component\": response_component,\n",
    "            }\n",
    "        )\n",
    "p.add_link(\"input\", \"rewrite_template\", src_key=\"query_str\", dest_key=\"query_str\")\n",
    "p.add_link(\"input\", \"rewrite_template\", src_key=\"chat_history_str\", dest_key=\"chat_history_str\")\n",
    "        \n",
    "# Linking rewrite_template to LLM, and LLM to retriever\n",
    "p.add_link(\"rewrite_template\", \"llm\")\n",
    "p.add_link(\"llm\", \"retriever\")\n",
    "\n",
    "# Linking retriever to reranker, and LLM to reranker\n",
    "p.add_link(\"retriever\", \"reranker\", dest_key=\"nodes\")\n",
    "p.add_link(\"llm\", \"reranker\", dest_key=\"query_str\")\n",
    "\n",
    "# Linking reranker to response_component\n",
    "p.add_link(\"reranker\", \"response_component\", dest_key=\"nodes\")\n",
    "\n",
    "# Linking input to response_component\n",
    "p.add_link(\"input\", \"response_component\", src_key=\"query_str\", dest_key=\"query_str\")\n",
    "p.add_link(\"input\", \"response_component\", src_key=\"chat_history\", dest_key=\"chat_history\")\n",
    "\n",
    "pipeline_memory = ChatMemoryBuffer.from_defaults(token_limit=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = QueryPipeline(\n",
    "    modules={\n",
    "        \"input\": input_component,\n",
    "        \"rewrite_template\": rewrite_template,\n",
    "        \"llm\": llm,\n",
    "        \"rewrite_retriever\": retriever,\n",
    "        \"query_retriever\": retriever,\n",
    "        \"join\": argpack_component,\n",
    "        \"reranker\": reranker,\n",
    "        \"response_component\": response_component,\n",
    "    },\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# run both retrievers -- once with the hallucinated query, once with the real query\n",
    "pipeline.add_link(\n",
    "    \"input\", \"rewrite_template\", src_key=\"query_str\", dest_key=\"query_str\"\n",
    ")\n",
    "pipeline.add_link(\n",
    "    \"input\",\n",
    "    \"rewrite_template\",\n",
    "    src_key=\"chat_history_str\",\n",
    "    dest_key=\"chat_history_str\",\n",
    ")\n",
    "pipeline.add_link(\"rewrite_template\", \"llm\")\n",
    "pipeline.add_link(\"llm\", \"rewrite_retriever\")\n",
    "pipeline.add_link(\"input\", \"query_retriever\", src_key=\"query_str\")\n",
    "\n",
    "# each input to the argpack component needs a dest key -- it can be anything\n",
    "# then, the argpack component will pack all the inputs into a single list\n",
    "pipeline.add_link(\"rewrite_retriever\", \"join\", dest_key=\"rewrite_nodes\")\n",
    "pipeline.add_link(\"query_retriever\", \"join\", dest_key=\"query_nodes\")\n",
    "\n",
    "# reranker needs the packed nodes and the query string\n",
    "pipeline.add_link(\"join\", \"reranker\", dest_key=\"nodes\")\n",
    "pipeline.add_link(\n",
    "    \"input\", \"reranker\", src_key=\"query_str\", dest_key=\"query_str\"\n",
    ")\n",
    "\n",
    "# synthesizer needs the reranked nodes and query str\n",
    "pipeline.add_link(\"reranker\", \"response_component\", dest_key=\"nodes\")\n",
    "pipeline.add_link(\n",
    "    \"input\", \"response_component\", src_key=\"query_str\", dest_key=\"query_str\"\n",
    ")\n",
    "pipeline.add_link(\n",
    "    \"input\",\n",
    "    \"response_component\",\n",
    "    src_key=\"chat_history\",\n",
    "    dest_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selon le Context Chunk 4, les politiques de soutien à la pêche en Algérie ont inclus :\n",
      "\n",
      "* Le Plan National du Développement de la Pêche et de l'Aquaculture (PNDPA) 2003-2007\n",
      "* Le Schéma Directeur de Développement des Activités de la Pêche et de l'Aquaculture (SDDAPA) 2025\n",
      "* La feuille de route Strat-Saïd (2012-2014)\n",
      "* Le plan Aqua-Pêche 2020\n",
      "\n",
      "Ces politiques ont visé à promouvoir le développement de la pêche et de l'aquaculture en Algérie, en mettant en place des mécanismes de soutien pour les pêcheurs et les entreprises du secteur.\n"
     ]
    }
   ],
   "source": [
    "response = pipeline._execute(\"Quelles ont été les politiques de soutien à la pêche en Algerie?\", pipeline_memory)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pipeline._execute(\"hello\", pipeline_memory=pipeline_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Bonjour! Comment puis-je vous aider aujourd'hui?\n",
      "assistant: La sécurité alimentaire en Algérie est un sujet complexe et multiforme. Selon les contextes fournis, il semble que la sécurité alimentaire en Algérie est confrontée à plusieurs défis. \n",
      "\n",
      "D'une part, les politiques agricoles et de soutien à l'agriculture ont été mises en place pour améliorer la sécurité alimentaire, mais leur efficacité et leur impact sont discutés. Le Plan National de Développement Agricole (PNDA) de 2000 visait à améliorer la sécurité alimentaire, mais son évaluation est difficile en raison du manque de données et de documents formels.\n",
      "\n",
      "D'autre part, les études et les recherches sur la sécurité alimentaire en Algérie sont nombreuses, mais elles montrent que le sujet est encore peu exploré, notamment en ce qui concerne les effets des politiques de soutien à l'agriculture et à la pêche sur la sécurité alimentaire.\n",
      "\n",
      "En outre, les défis liés à la sécurité alimentaire en Algérie sont nombreux, tels que la pénurie de main-d'œuvre agricole, l'urbanisation anarchique, la consommation des terres agricoles les plus fertiles, les difficultés d'approvisionnement en intrants, etc.\n",
      "\n",
      "En résumé, la sécurité alimentaire en Algérie est un sujet complexe qui nécessite une approche globale et une meilleure compréhension des défis et des opportunités. Les politiques et les stratégies mises en place doivent être évaluées et ajustées pour répondre aux besoins de la population et assurer une sécurité alimentaire durable.\n",
      "assistant: Your previous question was: How is food security in Algeria?\n"
     ]
    }
   ],
   "source": [
    "msg = [\"hello\", \"how is food security in Algeria?\", \"What was my previous question?\"]\n",
    "\n",
    "for msg in msg:\n",
    "    response = str(pipeline._run(msg, pipeline_memory=pipeline_memory))\n",
    "    response.replace(\"assistant:\", \"\").strip()\n",
    "    print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(pipeline.dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the network\n",
    "net.write_html(\"../data/html/2_qpchat_dag.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [phoenix.session.session] Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌍 To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "# setup Arize Phoenix for logging/observability\n",
    "import phoenix as px\n",
    "import llama_index.core\n",
    "\n",
    "px.launch_app()\n",
    "llama_index.core.set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
